---
title: "Machine Learning Project: TD3 TensorFlow"
date: 2020-07-08
tags: [machine learning, neural network, TD3, reinforcement learning]
tagline: "Using OpenAI environments"
header:
  overlay_color: "#000000"
  overlay_image: "/images/banner2.jpg"
  actions:
  - label: "View on Github"
    url: "https://github.com/vmorarji/TD3-TensorFlow"
excerpt: "Solving OpenAI's gym environments that have a continuous action space using TD3 in TensorFlow 2.x"
---


### TD3 for OpenAI gym environments using Tensorflow

To address the function approximation error of Actor-Critic methods, [Fujimoto et al.,](https://arxiv.org/abs/1802.09477) proposed the Twin Delayed Deep Deterministic policy gradient method (TD3).

Using a total of six neural networks, TD3 minimises the approximated Q-value by taking the minimum value from two Critic neural networks and uses this value to optimise the Actor network.

The [original implementation](https://github.com/sfujim/TD3) is written in Pytorch  and is the basis for the Tensorflow version written here.

### Requirements

Gym: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `pip install gym`

Pybullet: 	`pip install pybullet_envs`

The below line in `main.py` has been commented out. To enable recordings of certain episodes simply uncomment line 19.
```python
env = wrappers.Monitor(env, save_dir, force = True)
```
Note that one Windows machines ffmpeg will need to be saved in the same local directory as `train.py`.



### Results

After 1 million timesteps the model will achieve an average total reward of 2200 for "AntBulletEnv-v0".


![Alt Text](https://i.imgur.com/EURQj7q.gif)


The Actor loss has a local minimum around the -60 value where the agent will get stuck in a single position. Using Huber loss as opposed to MSE was instrumental in the agent progressing and achieving higher total rewards.

![Imgur](https://i.imgur.com/HosZ5xN.png)
